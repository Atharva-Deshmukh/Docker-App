INSIDE DOCKERFILE
--> To run the app, [node index.js]
--> docker can be run in any terminal since it is globally installed

DOCKER COMMANDS
--> To pull an image [docker pull <imageName>]
--> To run an image as a container [docker run -it --name <name of container u want> <image name> /bin/bash]
    it => interactive
    /bin/bash means after container is created, run a bash inside it
--> to run image as container along with port specification:
    docker run --name <name_of_container_u_want> -p <port_u_choose>:<port_of_app> <image_u_are_running>

--> to delete everything inside the local system => docker system prune -a
    This will remove:
  - all stopped containers
  - all networks not used by at least one container
  - all images without at least one container associated to them
  - all build cache


PROCESS TO WRITE THE dockerfile:
--> install the microsoft docker extension. this will enable intellisense for the code inside dockerfile
--> follow the commands in exact order as executed when creating this application
--> like first we installed the latest node image
--> dockerfile creates the image of this project
--> copy the code inside the docker image 
    command => COPY source_path dest_path
    . represents current directory, jaise yaha pe jaha dockerfile present hai (jis directory me), image me code usi directory ka copy hoga, means image ke andar jo code store hua vo image me root me store hoga
    . in destination means jo source se code uthaaya hai, vo image me bhi usi path pe rakho (in root directory of image)
    if the dest_path = ./some_folder, then the code in the root directory of the simpleApp will be stored in some_folder inside the image file
--> after creating the image and copying the source code, install all dependencies
    RUN npm install  (installs all dependencies from package.json)
--> now, application ko run honey ke liye ek port do, it must be same as that given in the source code (5500 in app)
    (Define the network ports that this container will listen on at runtime)
--> now give the command u use to run the project inside CMD, its in array format
    our command => node index.js

--> [[EXTRA]]  if apnaa code apney ko lets say kisi aur directory me karna hota (say /app), to sabhi commands ko /app me chalaana padtaa
    rather modifying individual commands, we first set the work directory to /app using WORKDIR command

--> ab dockerfile ready hai, uska build bnao using this cmd in vs code terminal: docker build -t <name_for_image> .
    t => tag (the name u want to give)
    . => root me bnaegi image, if u want to make image in some other folder, give ../folder_path
    

    [[NOTE]] run this command in that directory where there is dockerfile, better to run in vs code terminal
    [[NOTE]] image will be visible in docker desktop, not in the VS code

--> to list all images inside docker, run this in terminal (cmd), docker images

--> Run the image on port 5500, which is also the app port and type localhost 5500 on browser, u will see tha app running there

--> [[SCENARIO]] no port is given by default while running the container, the app runs in docker but you cannot access it on the browser
    hence we write EXPOSE <port_number> to be able to run this app on the browser as well

--> [[SCENARIO]] write 0 in the port number to generate any random port number so that if another container is running on say 5500, the randomly generated
    port number will be something else

DOCKERIGNORE FILE
--> it has the files which we don't want to include inside the docker image
--> make a file .dockerignore and ignore the notes.txt file and node_modules (since they are bulky)
--> since we already have npm install command in dockerfile to install the needed dependencies on the globally
--> to ignore all the text files in a single command, use .txt in 


IMAGE DELETION:
--> there are two ways: delete icon in docker desktop and via command line
--> but if the image is being used by any container (the container may be running or may not be running)
    then u can't directly delete that image using delete icon
--> to delete that image, u first need to delete the container/s using it.
--> to delete unused image using terminal => docker image rm <image_name>
--> to delete used image using terminal => docker image rm <image_name> -f

--> to list all containers => docker ps -a
    ps means processors
--> to delete a non-running container via terminal => docker container rm <container_name>
--> to delete a running container via terminal => docker container rm <container_name> -f
--> to delete a running container via desktop => pause the container and then delete using icon


IMAGE VERSIONING:
--> whenever introducing changes inside the dockerimage, u need to create a new image
--> either keep a completely different name to that updated image or use version 
--> Versioning is more feasible over here since its easy to track and comprehend
--> delete everything inside the docker desktop first and create a new image

--> if we don't create any version, we get tag = latest
--> to create new version of image => docker build -t <image_name>:v<version_no> .
--> we get tag = v<version_no>

VOLUMES IN DOCKER:
--> whenever we change the source code, the image is not changed since it was built before the changes were introduced
--> But to see the changes reflected in the container, we need to create a new image and run a new container. But this can 
    be optimised using volumes.
--> Docker volumes are a widely used and useful tool for ensuring data persistence while working in containers. Docker 
    volumes are file systems mounted on Docker containers to preserve data generated by the running container.
--> The major difference between a container and an image is the top writable layer. All writes to the container that 
    add new or modify existing data are stored in this writable layer. When the container is deleted the writable layer 
    is also deleted. The underlying image remains unchanged.
--> Because each container has its own thin writable container layer, and all changes are stored in this container layer, 
    this means that multiple containers can share access to the same underlying image and yet have their own data state.

OPTIONS DOCKER PROVIDE FOR FILE STORAGE IN HOST:

Docker provides two main options for containers to store files on the host machine so that the files persist even 
after the container stops:
--> Volumes: Docker volumes are the recommended way to persist data generated by and used by Docker containers. 
             Volumes are managed by Docker and exist outside the lifecycle of a container. They can be created and 
             managed independently of the container, allowing for data persistence even if the container is removed. 
             Docker volumes are flexible and can be shared among multiple containers, making them suitable for various 
             use cases.
--> Bind Mounts: Bind mounts allow you to link a directory on the host machine to a directory in the container's 
               filesystem. With bind mounts, files and directories are shared between the host and the container. 
               Changes made to files in the bind-mounted directory from either the host or the container are immediately 
               reflected in the other.Bind mounts offer flexibility and simplicity but lack some of the features provided 
               by Docker volumes, such as volume management and sharing between containers.

WHY WE NEED VOLUMES?

--> Persistence: By default, data within a Docker container's filesystem is ephemeral. When the container is stopped or 
    removed, any changes made to its filesystem are lost. Volumes provide a way to persist data beyond the lifetime of 
    a container. This is particularly important for databases, file uploads, log files, and any other data that needs 
    to be retained.
--> Sharing data between containers: Volumes can be shared among multiple containers, allowing them to access and modify 
    the same data. This is useful for scenarios such as sharing configuration files, static assets, or database files among 
    multiple containers.
--> Backup and Restore: With volumes, you can easily back up and restore data independently of containers. Since volumes 
    are external to containers, you can create backups of the volume data without needing to stop or interfere with running 
    containers.
--> Performance: Docker volumes can offer better performance compared to storing data directly within containers. 
    This is because volumes can be managed separately from containers, potentially allowing for optimizations such as using 
    optimized storage drivers or networked storage solutions.
--> Decoupling application and data: Storing data in volumes separates it from the application logic encapsulated within 
    containers. This separation makes it easier to manage and update applications independently of their data, providing more 
    flexibility and scalability.

HOW DOCKER VOLUMES ARE IMPLEMENTED IN THIS PROJECT:

--> install nodemon globally --> npm install -g nodemon
--> generally in a node application, changes are reflected after restart only. But with nodemon, this issue is solved.
--> now u just need to refresh instead of restart.
--> command to run the app with nodemon --> nodemon index.js
--> we will need nodemon inside our docker volumes as well, hence change the dockerfile accordingly
--> Also it is better to take the app inside the volumes in a directory, hence make a WORKDIR in dockerfile
--> new CMD to run the app inside the container --> nodemon index.js
--> command to create a new volume --> 

docker run --name basic-app-container -p 5500:5500 --rm -v C:/Users/z004r8tj/Desktop/SimpleApp:/app basic-app
    
    rm isiliye so that existing container should b removed and new one is created.
    v to create volume and give the ABSOLUTE path (NOT relative one) of the project after v
    BEFORE : yaha se data uthaayenge
    AFTER : yaha data rakhenge inside volume


COMPOSE FILE:

Docker Compose is a tool you can use to define and share multi-container applications. 
This means you can run a project with multiple containers using a single source.

Docker Compose is used to run multiple containers as a single service. 

For example, assume you're building a project with NodeJS and MongoDB together. 
You can create a single image that starts both containers as a service â€“ you don't need to 
start each separately.

u need to create compose.yaml file in the root

A Compose file is used to define how one or more containers that make up your application are configured. 
Once you have a Compose file, you can create and start your application with a single command: docker compose up

DOCKER COMPOSE.yam file

--> services: This section defines the services for your application.
--> image: Specifies the base Docker image to use for the service. 
--> build: give path where u want to create the build, use . if jaha hai vahi build hona to
--> container_name: Defines the name for the container created from this service.
--> ports: Maps port 5500 on the host to port 5500 on the container, allowing access to the Node.js application.